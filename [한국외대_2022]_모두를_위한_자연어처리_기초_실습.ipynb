{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[한국외대_2022]_모두를_위한_자연어처리_기초_실습.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9TOC7HKrcu-"
      },
      "source": [
        "# 모두를 위한 자연어처리 기초 실습 <br>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "####**강의자:** 고려대학교 박찬준<br>\n",
        "https://parkchanjun.github.io/\n",
        "\n",
        "* 전통적인 자연언어처리 실습 (Colab, Github, 텍스트 분석 실습)\n",
        "* 기계번역 관련 무한 질의 응답\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##전통적인 NLP는 형태소분석 / 구문분석 / 의미분석 / 화용분석 /담화분석 5단계로 구성됨\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "###형태소 분석\n",
        "* 형태소란 의미를 가지는 가장 최소의 단위 <br>영어 8품사 생각하면 이해하기 쉬움\n",
        "\n",
        "###구문분석\n",
        "* 명사구, 동사구 처럼 구문구조를 분석하는 것을 의미함 <br>\n",
        "* Tree를 만들어주는 거라고 이해하면 됨\n",
        "\n",
        "###의미분석\n",
        "* 배가 아프다.배를 먹는다 배를 타다. 이 3가지 배를 과연 컴퓨터가 분류할 수 있을까? <br>\n",
        "* 이것이 바로 의미분석 파트에서 담당하는 언어처리 기술임\n",
        "\n",
        "###화용론\n",
        "* 화용론에서는 발화 맥락에 따라 지시체가 바뀌는 ‘나’, ‘지금’, ‘여기’와 같은 지표등을 연구하는 분야 \n",
        "* 즉 맥락 속에서 사용되는 언어 형식의 사용 원리와 요소 등을 체계적으로 연구 <br>\n",
        "* 대표적으로 상호참조해결 Task가 있음\n",
        "\n",
        "###담화분석\n",
        "* 챗봇, 대화 시스템\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-hFajsE45Wn7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhlrf7WWt6De"
      },
      "source": [
        "## 어절 분리\n",
        "* 어절은 영어에서 띄어쓰기 단위라고 생각하면 이해하기 쉽습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC3rSTSjuGoo"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences=\"My name is park\"\n",
        "tokens=nltk.word_tokenize(sentences)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8vEQ2kRuzEO"
      },
      "source": [
        "## 문장 분리\n",
        "* NLTK로 문장 분리도 가능합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjACP6Ktuy0w"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentences=\"My name is park. How old are you.\"\n",
        "sen=nltk.sent_tokenize(sentences)\n",
        "print(sen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3dtjkc_vEPK"
      },
      "source": [
        "##형태소\n",
        "* 뜻을 가진 가장 작은 말의 단위\n",
        "\n",
        "\n",
        "* 입력으로 word_tokenize를 통해 토큰으로 나눈 Token 리스트를 넣어주어야 함<br>\n",
        "출력은 형태소 품사 태깅 된 결과가 나옴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlSkeHw2vQqJ"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence=\"my name is jack and i am 22 years old and my major is natural language processing\"\n",
        "\n",
        "tokens=nltk.word_tokenize(sentence)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "\n",
        "print(tagged) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b28W3K8Pvbct"
      },
      "source": [
        "##개체명 인식\n",
        "\n",
        "* 컴퓨터가 사람이름,장소,지역,조직체를 인식할 수 있을까?\n",
        "\n",
        "* 즉 Tom lives in NewYork이라는 문장이 있을 때 Tom이 사람이고 NewYork이 장소다는 것을 컴퓨터가 알아차릴 수 있을까?\n",
        "\n",
        "* 그것을 가능하게 하는 것이 바로 NER(Name Entity Recogniton)즉 개체명 인식임 <br><br>\n",
        "\n",
        "####Named Entity (개체명) : 사람, 조직, 장소 이름 등 이름을 가진 개체\n",
        "\n",
        "####Named Entity Recognition (개체명 인식) : 텍스트에서 개체명을 인식하고, 그 유형을 알려줌\n",
        "<br>\n",
        "\n",
        "* 개체명인식을 진행하기 위해선 input 데이터로 형태소태깅 된 리스트가 들어와야 됨\n",
        " \n",
        "* 즉 개체명인식을 진행하기 위해 먼저 nltk.word_tokenize를 진행하고 nltk.pos_tag를 진행한 후 이것을 input으로 넣어야 함을 알 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHoDfJu2v_po"
      },
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "sent=\"Mark is studying at Stanford University in California\"\n",
        "\n",
        "tokens=nltk.word_tokenize(sent)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "entities=nltk.chunk.ne_chunk(tagged)\n",
        "\n",
        "print(entities)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c2wgVaxwV70"
      },
      "source": [
        "## Edit Distance\n",
        "\n",
        "* NLTK를 통하여 Edit Distance에 대해서 살펴보려고 함\n",
        "\n",
        "* 간단히 생각해 2개의 단어가 얼마나 다르냐를 숫자로 표현해주는 것이 Edit Distance임\n",
        "\n",
        "* 예를 들어 CAT 과  HAT 두단어의 차이는 각 단어의 첫글자인 C와H 임\n",
        "\n",
        "* 즉 1개의 문자만이 차이가 남\n",
        "\n",
        "* 따라서 CAT과 HAT의 Edit Distance는 1 이되는 것 임\n",
        "\n",
        "* Edit Distance에는 3가지 연산이 있음\n",
        "\n",
        "\n",
        "#### Insertion\n",
        "#### Deletion\n",
        "#### Substitution\n",
        "\n",
        "\n",
        "* 즉 삽입, 삭제, 교체 총 이렇게 3가지 연산이 존재\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwtOuNCywx6z"
      },
      "source": [
        "import nltk \n",
        "from nltk.metrics import edit_distance \n",
        "\n",
        "print(edit_distance(\"CAT\",\"HAT\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEBQcfqMwzk1"
      },
      "source": [
        "## Stemming\n",
        "* stem이라는 영어단어 들어보셨나요?\n",
        "\n",
        "* (식물의) 줄기라는 뜻을 가진 영어단어 임.언어학에서는 stem을 어간이라고 함\n",
        "\n",
        "* 어간은 굴절하는 단어에서 변화하지 않는 부분을 의미함\n",
        "\n",
        "###스태밍(Stemming)이란 어간추출을 의미함\n",
        "\n",
        "* 쉽게 말해 형태가 변한 단어로부터 군더더기를 제거하고 그 단어의 원래 모습을 추출하는 것을 말함\n",
        "\n",
        "* 예를 들어 going이라는 단어가 있다면 Stemming을 진행할시 go\n",
        "\n",
        "* Computers라는 단어를 Stemming을 진행할 시 Comput 를 추출하는 과정을 Stemming이라고 함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRbW6Xv9xOzi"
      },
      "source": [
        "import nltk \n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "pst=PorterStemmer()\n",
        "\n",
        "print(pst.stem(\"computers\"))\n",
        "print(pst.stem(\"going\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIc7CdfRxSYM"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "* Lemmatization이란 문장 속에서 다양한 형태로 활용된(inflected) 단어의 표제어(lemma)를 찾는 일을 뜻함 <br> \n",
        "* 여기서 말하는 표제어란 사전에서 단어의 뜻을 찾을 때 쓰는 기본형이라고 생각하면 됨\n",
        "* 즉 Lemmatization은 단어의 원형을 추출해주는 녀석이라고 생각하면 이해하기 쉬움\n",
        "* 예를들어 is를 Lemmatization하면 be가 되고, ate을 Lemmatization하면 eat이 됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKxKMKpsxhJs"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wlem=WordNetLemmatizer()\n",
        "\n",
        "print(wlem.lemmatize(\"ate\",pos='v'))\n",
        "print(wlem.lemmatize(\"is\",pos='v'))\n",
        "print(wlem.lemmatize(\"are\",pos='v'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0TOQb8txnLH"
      },
      "source": [
        "## Stopword\n",
        "\n",
        "* 불용어는 인터넷 검색 시 검색 용어로 사용하지 않는 단어. 관사, 전치사, 조사, 접속사 등 검색 색인 단어로 의미가 없는 단어임\n",
        "\n",
        "\n",
        "{'above', 'doing', 'too', 'can', 'd', 't', 'then', 'what', 'same', 'himself', 'but', 'with', 'on', 'when', 'so', 'isn', 'his', 'further', 'been', 'being', 'our', 'because', 'are', 'from', 'mustn', 'at', 'between', 'here', 'most', 'ours', 'again', 'shouldn', 'have', 'both', 'below', 'against', 'few', 'wasn', 'those', 'hadn', 'once', 'don', 'ain', 'for', 'under', 'o', 're', 'yourselves', 'them', 'themselves', 've', 'about', 'your', 'ourselves', 'who', 'after', 'or', 'he', 'over', 'this', 'how', 'myself', 'into', 'in', 'such', 'aren', 'hasn', 'before', 'whom', 'won', 's', 'were', 'only', 'herself', 'we', 'that', 'was', 'had', 'no', 'of', 'during', 'down', 'has', 'off', 'while', 'where', 'a', 'if', 'until', 'weren', 'be', 'having', 'theirs', 'doesn', 'will', 'to', 'just', 'her', 'ma', 'll', 'there', 'and', 'does', 'other', 'their', 'own', 'why', 'itself', 'its', 'each', 'by', 'not', 'she', 'some', 'him', 'very', 'm', 'should', 'now', 'couldn', 'yourself', 'these', 'as', 'didn', 'an', 'nor', 'is', 'yours', 'did', 'the', 'do', 'my', 'all', 'needn', 'y', 'which', 'up', 'shan', 'haven', 'through', 'me', 'out', 'mightn', 'wouldn', 'they', 'i', 'you', 'hers', 'it', 'more', 'any', 'am', 'than'}\n",
        "\n",
        " \n",
        "\n",
        "* 영어에 불용어의 종류로는 이렇게 많은 단어들이 있으며 대부분 검색 시 의미없는 단어들임"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p01BcNqUxuz0"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop=set(stopwords.words('english'))\n",
        "print(stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TOCpR5Jx4Xt"
      },
      "source": [
        "## 문장이 주어졌을 때 불용어를 제외한 단어들만 추출하는 방법\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPY5Sxdox6kF"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "stop=set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "sen=\"I want to go to shopping and a I want to buy some of snack\"\n",
        "tokens=nltk.word_tokenize(sen)\n",
        "\n",
        "clean_tokens=[]\n",
        "for tok in tokens:\n",
        "  if len(tok.lower())>1 and (tok.lower() not in stop):\n",
        "    clean_tokens.append(tok)\n",
        "\n",
        "  \n",
        "print(\"불용어 포함: \",tokens) \n",
        "print(\"불용어 미포함: \",clean_tokens)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi8iE-vtyAUw"
      },
      "source": [
        "## 특정한 품사만 추출해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUJXau5HyDiy"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sen=\"Barack Hussein Obama II is an American politician who served as the 44th President of the United States from 2009 to 2017.\"\n",
        "\n",
        "tokens=nltk.word_tokenize(sen)\n",
        "tagged=nltk.pos_tag(tokens)\n",
        "\n",
        "allnoun=[]\n",
        "for word, pos in tagged:\n",
        "  if pos in ['NN', 'NNP']:\n",
        "    allnoun.append(word)\n",
        "    \n",
        "print(\"형태소 분석 결과: \",tagged)\n",
        "print(\"명사만 추출: \", allnoun)\n",
        "\n",
        "allverb=[]\n",
        "for word, pos in tagged:\n",
        "  if pos in ['VBZ','VBD']:\n",
        "    allverb.append(word)\n",
        "\n",
        "print(\"동사만 추출\",allverb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD9YgojCyMWQ"
      },
      "source": [
        "## HTML 크롤링 후 단어 빈도수 구해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCDsEi0gyQE2"
      },
      "source": [
        "import nltk\n",
        "import urllib\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords  \n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "response=urllib.request.urlopen('http://python.org/') #웹에 정보를 요청한 후, 돌려받은 응답을 저장하여 ‘응답 객체(HTTPResponse)’를 반환한다.\n",
        "html=response.read()\n",
        "clean=BeautifulSoup(html,'html.parser').get_text() #html 코드 정제 진행\n",
        "\n",
        "tokens=[]\n",
        "for tok in clean.split():\n",
        "  tokens.append(tok) #하나의 리스트로\n",
        "\n",
        "stop=set(stopwords.words('english')) #불용어 \n",
        "\n",
        "clean_tokens=[]\n",
        "for tok in tokens:\n",
        "  if len(tok.lower())>1 and (tok.lower() not in stop): #길이가 1 이상 인 것 !! stop word가 아닌 것 !\n",
        "    clean_tokens.append(tok)\n",
        "\n",
        "Freq_dist_nltk=nltk.FreqDist(clean_tokens) #FreqDist 클래스는 문서에 사용된 단어(토큰)의 사용빈도 정보를 담는 클래스이다.\n",
        "Freq_dist_nltk.plot(30, cumulative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RNAKmq9yc_t"
      },
      "source": [
        "## 명사만 추출해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB5eNascyb0M"
      },
      "source": [
        "import nltk\n",
        "import urllib\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords  \n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "response=urllib.request.urlopen('http://python.org/') \n",
        "html=response.read()\n",
        "clean=BeautifulSoup(html,'html.parser').get_text()   \n",
        "\n",
        "tokens=[tok for tok in clean.split()]\n",
        "\n",
        "stop=set(stopwords.words('english')) \n",
        "\n",
        "clean_tokens= [tok for tok in tokens if len(tok.lower())>1 and (tok.lower() not in stop)] \n",
        "\n",
        "tagged=nltk.pos_tag(clean_tokens)\n",
        "\n",
        "allnoun=[]\n",
        "for word,pos in tagged:\n",
        "  if pos in ['NN','NNP']:\n",
        "    allnoun.append(word)\n",
        "    \n",
        "Freq_dist_nltk = nltk.FreqDist(allnoun) \n",
        "Freq_dist_nltk.plot(30, cumulative=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai5LijaLvDG_"
      },
      "source": [
        "## 한국어 형태소분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUmgKgMQvC3e"
      },
      "source": [
        "# konlpy 패키지 다운로드\n",
        "# Error 발생 시 다시 실행\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NskKEBhfvLA2"
      },
      "source": [
        "# konlpy 관련 패키지 import\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Hannanum\n",
        "from konlpy.tag import Komoran\n",
        "from konlpy.tag import Twitter\n",
        "\n",
        "kkma = Kkma()\n",
        "okt = Okt()\n",
        "komoran = Komoran()\n",
        "hannanum = Hannanum()\n",
        "twitter = Twitter()\n",
        "\n",
        "\n",
        "# konlpy 중 Kkma는 문장 분리가 가능 (다른 라이브러리는 되지 않음)\n",
        "print (\"kkma 문장 분리 : \", kkma.sentences('네 안녕하세요 반갑습니다.'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELnqvwEOvTnG"
      },
      "source": [
        "# konlpy 의 라이브러리 형태소 분석 비교\n",
        "print(\"okt 형태소 분석 :\", okt.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"kkma 형태소 분석 : \", kkma.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"hannanum 형태소 분석 : \", hannanum.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"komoran 형태소 분석 : \", komoran.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"twitter 형태소 분석 : \", twitter.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7cp8u2lwpDx"
      },
      "source": [
        "# konlpy 의 라이브러리 품사태깅 비교\n",
        "print(\"okt 품사태깅 :\", okt.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"kkma 품사태깅 : \", kkma.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"hannanum 품사태깅 : \", hannanum.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"komoran 품사태깅 : \", komoran.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n",
        "print(\"twitter 품사태깅 : \", twitter.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##번역 평가에 사용되는 의미적 유사도\n",
        "\n",
        "모델이 실제로 번역한 문장이 실제 정답과 동일한 의미이지만, 단순 단어 겹침 (overlab) 만으로 평가하는 것이 불가능할 수 있다. \n",
        "\n",
        "예를 들어,\n",
        "\n",
        "  * 모델이 번역한 문장: *There are the rats eating man's foods.*\n",
        "\n",
        "  * 실제 정답: *There are mice that eat human food.*\n",
        "\n",
        "위 문장의 경우에 실제 두 문장의 의미는 거의 유사하지만, 단어 겹침이 많이 발생하지 않는다.\n",
        "\n",
        "이러한 문제는 현재 모델이 번역을 얼마나 잘 하는지 성능을 측정하는 것에 큰 방해물이 된다. 이러한 극복하기 위해서, 실제 정답과 모델이 번역한 문장의 실제 의미적 유사도를 측정하는 방법 혹은 **모델**이 필요하다."
      ],
      "metadata": {
        "id": "KdONTd2m2xsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**모델**을 사용해서, 의미적 유사도를 측정하기 위한 방법은 크게 두 가지로 나눌 수 있다.\n",
        "\n",
        "\n",
        "####Sentence embedding similarity (문장간 유사도) : \n",
        "- Laser Embedding score\n",
        "\n",
        "####Token embedding similarity (단어간 유사도) : \n",
        "- BERT score\n",
        "\n",
        "이번 실습에서는 문장간 유사도인 Laser Embedding score를 다루고, BERT score는 생략한다."
      ],
      "metadata": {
        "id": "22i5RfYoUnN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT score\n",
        "\n",
        "BERT score는 단어 간의 의미적 유사도의 연산 (곱, 평균, log 곱) 등을 이용해서, 실제로 유사한 의미의 단어가 사용되었는지를 통해 모델이 생성한 문장과 실제 정답 문장간의 유사도를 BERT의 embedding을 통해 계산한다.\n",
        "\n",
        "가령,\n",
        "  * 모델이 번역한 문장: *There are the rats eating man's foods.*\n",
        "\n",
        "  * 실제 정답: *There are the mice that eat human food.*\n",
        "\n",
        "\n",
        "위 두 문장에서 rats와 mice, man과 human은 비슷한 의미를 가지고 있지만, 단어 겹침을 통해 평가를 하면, 둘은 전혀 다른 단어로 해석되어 점수에 반영되지 않는다. 하지만, BERT score는 의미적으로 유사한 단어간의 점수를 높게 계산할 수 있어서 유사한 단어를 점수에 반영할 수 있다.\n"
      ],
      "metadata": {
        "id": "t9PJOiszUuZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Laser embedding score\n",
        "\n",
        "실제 문장의 의미가 동일한 벡터 공간에서 유사한 위치에 있는지를 통해 score를 계산한다. Laser embedding score는 단어간의 의미적 유사도가 아닌 문장의 의미적 유사도로 평가를 진행하기 때문에 어순이나 단어 변경에 민감하지 않다. \n",
        "\n",
        "\n",
        "가령,\n",
        "  * 모델이 번역한 문장: *There is a mouse that is being bully by a cat.*\n",
        "  * 실제 정답: *There is a cat that annoys a mouse.*\n",
        "\n",
        "위 두 문장에서 어순과 사용된 단어가 다를지라도, 비슷한 문장은 가까운 벡터 공간에 매핑되어 높은 벡터간 유사도를 가지게 된다. 즉, 높은 문장 유사도를 가지게 된다."
      ],
      "metadata": {
        "id": "dfsqU1MXKYsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# laser score 계산 라이브러리 다운\n",
        "!pip install laserembeddings[en]\n",
        "!python -m laserembeddings download-models"
      ],
      "metadata": {
        "id": "7e-Elzjp4amG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from laserembeddings import Laser\n",
        "import numpy as np\n",
        "\n",
        "laser = Laser()\n",
        "\n",
        "s1 = 'let your neural network be polyglot'\n",
        "s2 = 'use multilingual embeddings!'\n",
        "\n",
        "# get laser embedding vectors\n",
        "embeddings = laser.embed_sentences([s1, s2],lang='en')\n",
        "\n",
        "# 두 문장의 벡터 값 확인\n",
        "embeddings"
      ],
      "metadata": {
        "id": "gPJu_dM82vt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_similarity(a, b):\n",
        "    return np.dot(a, b.T).squeeze() / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "sen1_laser_emb = embeddings[0]\n",
        "sen2_laser_emb = embeddings[1]\n",
        "\n",
        "# cosine similarity를 통해 두 문장 벡터 유사도 계산 \n",
        "cos_similarity(sen1_laser_emb, sen2_laser_emb)"
      ],
      "metadata": {
        "id": "5npddty84Jfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = \"The man is giving the snake to the mouse.\"\n",
        "\n",
        "s2 = [\n",
        "        \"oh my god, Look at that snake!\",\n",
        "        \"A child is riding a horse.\",\n",
        "        \"The man wearing a hard hat is dancing.\",\n",
        "        \"The man is feeding a mouse to the snake.\",\n",
        "    ]\n",
        "\n",
        "sen1_laser_emb = laser.embed_sentences(s1, lang='en')\n",
        "\n",
        "print(f'원본 문장: {s1}\\n')\n",
        "\n",
        "for tgt in s2:\n",
        "  sen2_laser_emb = laser.embed_sentences(tgt, lang='en')\n",
        "  print(f'번역 문장:{tgt}, 원본 문장과 번역 문장의 유사도: {cos_similarity(sen1_laser_emb, sen2_laser_emb)}')  "
      ],
      "metadata": {
        "id": "s9VnCaaH2v6K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}